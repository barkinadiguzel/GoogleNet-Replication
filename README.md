# ğŸŒ GoogleNet-Replication PyTorch Implementation

This repository contains a PyTorch replication of **GoogLeNet (Inception v1)** including **auxiliary classifiers** and **dimension reduction in Inception modules** for efficient computation.

- Implemented full **GoogLeNet architecture** with auxiliary classifiers.  
- Architecture follows:  
**Conv1 â†’ MaxPool â†’ Conv2 â†’ MaxPool â†’ Inception3a â†’ Inception3b â†’ MaxPool â†’ Inception4a â†’ Inception4b â†’ Inception4c â†’ Inception4d â†’ Inception4e â†’ MaxPool â†’ Inception5a â†’ Inception5b â†’ AvgPool â†’ Dropout â†’ Flatten â†’ FC**  
**Paper:** [Going Deeper with Convolutions (GoogLeNet)](https://arxiv.org/abs/1409.4842)

---

## ğŸ› Overview â€“ GoogLeNet Architecture

![Figure 1](images/figmix1.jpg)  

- **Figure 1:** Full GoogLeNet model with Inception modules and auxiliary classifiers.  
- **Figure 2:** Inception module detail showing parallel 1Ã—1, 3Ã—3, 5Ã—5 convolutions, 1Ã—1 dimension reduction, and pooling projection.  
- **Table 1:** Layer dimensions, kernel sizes, and number of parameters in each stage of GoogLeNet.  
- **Table 2:** FLOPs and memory estimates for each layer.  
- **Table 3:** Configuration of Inception modules (3a, 3b, 4aâ€¦5b) with channel counts and reductions.  
- **Auxiliary Classifiers:** Mid-network classifiers providing additional gradient signal during training.  
- **Pooling Layers:** MaxPooling reduces spatial dimensions; Global Average Pooling prepares feature maps for FC layers.  
- **Final Layers:** Dropout â†’ Flatten â†’ Fully Connected layer for classification into 1000 classes.

> GoogLeNet is a deep CNN that uses Inception modules to extract multi-scale features efficiently. 1Ã—1 convolutions reduce dimensions to save computation, while auxiliary classifiers provide extra gradient signals to stabilize training. Spatial size is reduced with pooling, and the final features go through global average pooling, dropout, and a fully connected layer for classification. This design allows the network to be both deep and wide without excessive computational cost.



---

## ğŸ— Project Structure

```bash
GoogleNet-Replication/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py          # BN â†’ ReLU â†’ Conv2d
â”‚   â”‚   â”œâ”€â”€ inception_module.py    # 1x1, 3x3, 5x5 convolutions + pooling
â”‚   â”‚   â”œâ”€â”€ pool_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ maxpool_layer.py   # MaxPooling
â”‚   â”‚   â”‚   â””â”€â”€ avgpool_layer.py   # Global Average Pooling
â”‚   â”‚   â”œâ”€â”€ flatten_layer.py       # Conv â†’ FC transition
â”‚   â”‚   â””â”€â”€ fc_layer.py            # Fully Connected Layer
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â””â”€â”€ auxiliary_classifier.py  # Auxiliary classifier
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ googlenet.py           # Full GoogLeNet model
â”‚   â”‚
â”‚   â””â”€â”€ config.py                  # Hyperparameters: channels, kernel sizes etc.
â”‚
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
